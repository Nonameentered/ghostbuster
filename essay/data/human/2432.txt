When designing a communications system or defining a communications standard, a model
would be used to see test how different variations would perform in certain scenarios. This
project built up such a model to compare a range of modulation schemes and error protection
methods. The knowledge gained from this process enabled the full analysis of a complete
802.11a model at the end of the project.
The main performance gauge used in this project was a systems ability to cope with channel
noise. 3 types of channel noise were studied, additive white Gaussian noise, flat fading and
dispersive fading.
BPSK, QPSK, 16-PSK, 16-QAM and 64-QAM modulation schemes were compared, using
bit error rate curves and packet error rate curves. Bit error rates were calculated by comparing
the demodulated message with the original source message. Packet error rates were calculated
at the receiver using a 16-bit CRC added to the end of each packet.
The system model also includes convolutional coding, a type of forward error correction. Two
types of codes were analysed. A simple 3,[7,5] code was used to explain the principles of
convolutional coding, but the main focus of the project was on 7,[171,133] NASA coding,
which is very common. Messages were decoded using a Viterbi decoder.
The 802.11a model, supplied as a demo with MatLab, brings together all the system
components discussed in this project, along with a multi-carrier multiplexing technique
known as orthogonal frequency division multiplexing (OFDM).
The model was implemented in Simulink. Explanations and justifications for each component
of the model are given.
The conclusions section of this report covers common issues and decisions that are faced
when designing a communications system, and some recommendations for further modelling
work are given at the end.
As the majority of communications head into digital territory, digital signal processing is playing an ever larger role. Radio frequencies are becoming more crowded and services are requiring greater data rates, resulting in the realization of more complex modulation and coding schemes. Several stages of modulation and coding are often combined to maximise performance, and all this must be done inside a digital signal processor.
During the development of a communications system the manufacturer hopes to achieve:
Clearly the above requirements conflict, and there must be a trade-off. A manufacturer will use a system model to examine what modulation and coding schemes are available to give the best balance of the requirements.
This project will build a complete model of the physical layer of a communications system, from a serial binary source to a serial binary output at a receiver, and analyse the complete 802.11a wireless LAN model.
This section of the report details the implementation of the system model. Simulink was the obvious choice of implementation tool, since many of the complex coding tasks are provided in ready-to-use blocks. The model was built up in stages so that each part could be checked separately.
It has been seen that the encoding and modulation schemes chosen can have serious implications on the performance of a communications system. There are many more options than the various ones explored in this project, but these ones are currently used in wireless netorking (IEEE802.11), and digital radio/television broadcasting, which makes them obvious choices to describe. Each scheme has its merits and disadvantages, it is not so clear cut that the scheme that allows the fastest data rate can be regarded as the best.
If reliable transmission is required at a low SNR (e.g. less than 5dB), there would be little choice available and, out of the modulation schemes used here, BPSK would be used. This will give the slowest data rate, but it would be unrealistic to expect a fast data rate with such poor signal quality. The users of an 802.11a LAN system will often experience far lower data rates than promised by the equipment manufacturers, due to these occasions when the system must switch to its last resort BPSK modulation in order to guarantee error free transmission.
Newer wireless standards 802.11b and 802.11g have provision for even worse noise conditions, by employing DBPSK (Differential BPSK) and DQPSK, which offers a reduction in complexity with only a slight loss in performance. DQPSK is also used in DAB (Digital Audio Broadcasting). Further work could perhaps examine these differential modulation techniques in more detail.
At the opposite end of the spectrum, some communications require high data rates but can tolerate a lot of errors. The DVB standard defines the modulation and coding schemes that can be used, and offers a lot of flexibility. Some commercial digital television channels will use 64-QAM and puncturing to reduce the amount of bandwidth required by their signal (reducing the licensing costs). Unfortunately, the MPEG2 compression used in DVB does not have the ability to degrade gracefully and so in bad weather these channels can be virtually unwatchable.
Another factor which affects the choice of modulation is whether the system will have the ability to request retransmissions. In the performance analysis section, Table 3 and Figure 4.8 showed that the 802.11a standard favours higher data rates over 100% accurate data reception. This is tolerable due to its ability to request retransmission of failed data, meaning that no information can be lost. This approach can only be used where delivery time does not need to be guaranteed. In a time critical system, delivery time is essential and it would be unacceptable to need to retransmit data. These situations warrant the use of more complex codes and modulation schemes to try and salvage as much data from a signal as possible.
In the past, when decided which coding and modulation schemes to use for a project, the complexity would have been an important factor. More complex algorithms are more expensive to implement, and slower to execute. Slow execution may not be such an issue in broadcast, where a delay would not be noticed, but any delay in mobile telephone conversation would be a problem. Fortunately, the decreasing cost of hardware decoders and the abundance of available software decoders has resulted in a wave of solutions that can implement complex schemes at low cost. Digital TV set-top boxes and mobile telephones are two obvious examples.
Power considerations are also now being overcome, with mobile communications companies working hard to reduce the power consumption for these modulation and coding schemes. The fact that OFDM is a likely candidate for 4G mobile communications demonstrates this well.
The debate over which modulation schemes are best is ongoing.
This project offered a look at 5 of the most common modulation techniques, and only 2 convolutional codes. There are many others of equal merit, and further work could include these. Two examples raised in the Conclusion are DBPSK and DQPSK.
Wireless LAN only uses convolutional coding, but other wireless communications models include other types of error protection (outer coding) to protect against different types of errors, e.g. sudden bursts of noise. There are also methods of interleaving, whereby bits from different packets are shuffled before modulation. Outer coding and interleaving can, in extreme circumstances, help to restore a packet of data from an extremely violent burst of noise. Further work could perhaps look at the DVB model, which includes both outer coding and interleaving.
802.11 is well known to not perform at the data rates that should be possible. Aside the reasons already discussed, the other layers of the 802.11 standard play a big part. A true 802.11 model would need to take into account the interaction between devices using the correct protocol.
One final factor that would be taken into account while defining a system, is user perception of corrupt data. How many errors are acceptable to the human ear/eye. As previously stated, data compressed by an MPEG2 algorithm will suffer badly, as errors will be propagated for a period of time. However, the compression could be increased if this was likely to happen frequently. Uncompressed data may have the ability to degrade gracefully, giving the impression that it has been received correctly. However, human perception is difficult to model.