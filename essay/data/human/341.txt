The difference between the algorithms is the method used to guess a target item: one chooses an item in the midpoint of a range, whereas the other picks a random item within the range. We show that the former converges faster to the correct answer than the latter, as predicted by complexity theory.
Searching for a given item in a finite list is a common task in every day life. One searches for telephone numbers in a phonebook or words in a dictionary. In order to automate this process, one might create a computer program that runs sequentially through every word in the dictionary and asks the user whether the word in turn is the one she is looking for. Although this procedure would eventually find the target word, it would take a long time to do so. The time required to complete the task will depend on the relative position of the word in the dictionary: words in the beginning will be found faster than words toward the end. In fact, if the dictionary contains And the user would have to reply
Fortunately, there is a way to improve the efficiency of the look-up procedure: pick a word in the dictionary and ask whether the target word is further up or down in the sequence; if it is up, delete the lower part; if it is down, delete the upper part; do the same until the target word is reached. This procedure is called As intuitive as it seems, however, the Binary Search algorithm depends crucially on the criterion used to split the search space. Depending on the criterion chosen one might end up either with the optimal solution or the worst.
In this paper, we present a comparison between two algorithms that use Binary Search as their core procedure but differ slightly in the criterion used to split the search space: the first ( This difference is sufficient to change the worst-case complexity of the first algorithm from In order to empirically test that, we compare the performance of both algorithms in a number-guessing task. We hypothesise that
The main hypothesis is that Specifically, the mean number of guesses in the first case should be sistematically smaller than in the second case. As Table 1 shows, In addition,
The difference between the two algorithms becomes even more dramatic when one investigates their asymptotic behaviour. The ratio between the mean number of guesses of The reason is that, for the former, the number of guesses will linearly increase with the size of the range, whereas for the latter, the number of guesses will increase slowly, following a logarithmic function. Figure 1 illustrates this behaviour: as expected, the two curves diverge as the range size increases (from range 1-800 onwards).
We have empirically tested two Binary Search algorithms and showed that, as expected by complexity theory, The result illustrates the idea that seemingly minor differences among algorithms can yield large differences in performance, ranging from the worst to the optimal solution.
The results from This was expected, as the split points differ from run to run. While the first guess in
A possible criticism to the experiment is that the targets were not kept constant across algorithms (see Appendix for a sample output). What if the targets for Because the 100 targets in each experimental batch were chosen randomly, that possibility seems unlikely. Conversely, if targets were equal across conditions, then a similar point could be raised: that the targets could be easier for either condition. Once again, randomisation would render that possibility unlikely. Therefore, both designs are equivalent..