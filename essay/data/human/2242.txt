The Izhikevich Neuron model is currently the forefront spiking neural net model. It can emulate all the key modes of firing, and does so with only 13 floating point operations per second of emulation time 1. The Hodgkin's and Huxley model - the only other model capable of displaying so many firing modes - requires 1200. In an experiment done by Izhikevich, where he ran a network of these neurons with 10 10 synapses on a Beowulf supercomputer, it took 50 days to run what could be considered one second of brain activity. The integrate-and-fire model, the fastest spiking neuron model, which has drastically lower functionality, still requires 5 FLOPS and thus would have still taken around 20 days to run the simulation. This highlights a problem with such neurons - running them on conventional computers - even supercomputers, is not efficient. The only way to run them, at any kind of reasonable speed is on massively parallel hardware dedicated to running them. This report charts the development of a simulator for a theoretical board which could run Izhikevich type neurons, in a parallel fashion with easily expandable processing capabilities.
When all is said and done, I am very proud of this project, even though it is not really complete and has much work left to go on it. It has been a fascinating task and I have learnt a huge amount from working on it. When I embarked upon it, my knowledge of spiking neural networks was next to non-existent and it was the perfect way to learn more about them. While battling through mountains of data trying to debug it did get extremely frustrating, it has given me a good, clear understanding of the processes that occur within the neurons.
Due to the enourmous amounts of data generated, and calculations done by the program it forced me to deal with certain issues I had never had to consider before in my time as a programmer. How to optimize the speeds of processes, selecting functions that can operate at the lowest order of time possible - never before had I had the need to really consider these things, and it brought into perspective all that I have learnt recently in the algorithms module. And while last programs of mine have generated multidimensional, dynamic arrays of regularly changing data - it has never quite been on this scale before. The methods I found and used to order my thoughts, and the data in such a way that I could handle debugging was an important lesson for me.
While a few of the requirements and parts of the specification have not quite been met - for instance, the interface is not quite up to the standard suggested, only four types of neurons are possible and a layered topology is not implemented, I consider this to be simply part of the nature of the program. There will always be more work that can be done with it - and as soon as I can find the time, I have every intention of continuing through with it, to add as many of them in as I can.
There are many more features that I wish to add to it yet. A full SDL interface, real time graph plotting of spikes, multithreading for the different neurons, more user control over network setup and types of neurons and different ways of entering input are just the start of what I want to do with them - and they can only really be done once I've filled in the holes within the current specification.
There are still two key bugs within the program, but these should be easily dealt with. I will be removing the PCI interface from my version, and returning it to a purely software oriented program - this will remove the 'no-secondary-fire' problem from the second version. The other problem - that synaptic weights quickly reach their limits, can be dealt with by making changes to them occur through the use of a derivative, which slows down the rate at which they change, stretching it slowly across a few seconds.
I do feel that the level and range of the actual programming demonstrated within the program was unfortunately fairly low, but due to the huge amount of research, testing and debugging required I did not find time to really push the limits, as this would have required a greater range of functionality to make room, and require a wider range of programming skills.
If I was to do it again I would make it easier to perform the testing. By adding in a separate function or so, which could adjust certain values, and output strings of values, which again and again I found myself having to adjust and code for, I think I could have cut a large amount of the development time. But as they say - we live and learn. Or more precisely - our neural nets do.