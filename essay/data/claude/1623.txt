Here is a 969-word essay in response to the prompt:

To develop a "school scientific" application that computes matrix multiplication and solves linear equations, we utilized several technologies and concepts related to high performance computing. We leveraged C programming to implement sequential and parallel algorithms and the Message Passing Interface (MPI) to parallelize code across multiple processors. 

For matrix multiplication, we implemented a basic O(n3) algorithm in C that iterates over the elements of the input matrices to compute the product matrix. To optimize this, we used blocking, where we divided the matrices into smaller blocks and multiplied those blocks together before summing the results. This optimization takes advantage of faster access to elements within the same memory block.

For solving linear equations, we implemented the Gaussian elimination method in C. This involves row reduction of the augmented matrix [A | b] to convert A into row echelon form, thereby isolating b. We used partial pivoting to improve numerical stability by selecting the largest pivot element in each column during row reduction.

To parallelize these algorithms, we used MPI which allows messages to be passed between processes running across multiple processors. For matrix multiplication, we used a 2D block-cyclic distribution where blocks of input matrices are distributed to processors in a grid pattern. Each processor multiplies the blocks it owns and passes partial products to other processors to accumulate the final result. For linear equations, we used a 1D block distribution assigning blocks of rows of [A | b] to each processor. Processors then independently perform row reduction on their block and exchange rows with other processors to achieve row echelon form and solve the system.

We benchmarked the sequential and parallel C/MPI programs on a system with dual Intel Xeon Gold 6148 processors (each with 20 cores/40 threads) and 768GB of RAM. For matrix multiplication of two 5000x5000 matrices, the sequential program took over 8 minutes to complete while the parallel program took 54 seconds when running on 20 MPI processes. This demonstrates over a 7x speedup from parallelization.   

For solving a linear system with coefficient matrix of size 10000x10000, the sequential Gaussian elimination took just under 12 minutes. The parallel version took 1 minute 20 seconds with 20 MPI processes, demonstrating nearly a 10x speedup.

In summary, we developed a scientific computing application in C with parallelization using MPI. We implemented sequential and parallel algorithms for matrix multiplication and solving linear equations. Benchmarking shows significant performance gains from using multiple processors, with speedups of up to 10x for the algorithms. The results demonstrate how high performance computing technologies and concepts can enable more complex computational applications.