Robotic arms require accurate position sensing and feedback to function effectively. There are a few methods that robotic arms use to determine their position and orientation. 

One method is through the use of joint encoders. Joint encoders are sensors that track the angle of each joint in the robotic arm. By knowing the angles of all the joints, the position of the end effector (the end of the arm) can be calculated using forward kinematics. Forward kinematics uses the lengths of each link in the arm and the joint angles to geometrically calculate the end effector position. However, joint encoders require precise calibration to be accurate and any joint flex or other hardware changes can introduce errors. They also do not provide orientation information about the end effector.

Another approach is to use potentiometers or linear variable differential transformers (LVDTs). These sensors measure the linear motion of the joints in the arm. By combining multiple of these linear motion sensors, the full position and orientation of the end effector can be calculated. However, like joint encoders, these sensors require very precise calibration and are subject to inaccuracies from joint flex or other hardware changes. They can also be more complex to implement than joint encoders.

Some robotic arms use inertial measurement units (IMUs) to determine position and orientation. IMUs contain accelerometers, gyroscopes, and sometimes magnetometers to track the motion and orientation changes of the arm. By combining IMU data with an initial reference point, the position and orientation of the end effector can be estimated. However, IMU data is subject to drift over time and also requires filtering to account for errors, so IMUs typically need to be combined with another absolute position reference periodically.

Many modern robotic arms use vision systems or external cameras to determine position and orientation. Computer vision algorithms track the end effector and joints of the arm visually and compare them to a model of the arm's geometry to calculate its pose. Vision has the advantage of not requiring precise calibration like the other methods but requires clear sight lines and may have errors from visual occlusions, lighting changes, and camera calibration. Multiple cameras from different angles can help improve accuracy.

In many advanced robotic arms, a combination of multiple position sensing methods are used together for increased accuracy and precision. For example,  a combination of joint encoders, IMUs, and vision systems are often employed. By fusing the data from multiple complementary 
sensors, many of the individual downsides can be mitigated to enable highly accurate position and motion control of the robotic arm.

In summary, the key methods for robotic arms to determine position and orientation are: 1) Joint encoders to track joint angles, 2) Linear motion sensors like potentiometers to track joint translations, 3) IMUs to track motion and orientation changes, and 4) Vision systems to visually detect the arm pose. Used together, these methods can provide robotic arms an accurate sense of their position and enable precise control of their motion.